#!/usr/bin/env python3
"""
Git Upload Verifier - ë¡œì»¬ í”„ë¡œì íŠ¸ Git ì—…ë¡œë“œ ì „ ìë™ ì ê²€ ë„êµ¬

ê¸°ëŠ¥:
1) íŒŒì¼ í•´ì‹œ(SHA-256)ë¡œ ì¤‘ë³µ ì‹ë³„Â·ì‚­ì œ(ê°€ì¥ ìµœì‹  mtime 1ê°œë§Œ ë³´ì¡´)
2) ìœ„í—˜/ë¶ˆí•„ìš” íŒŒì¼ íŒ¨í„´ ì œì™¸(.env, *.key, node_modules, __pycache__, *.tmp, *.bak, .DS_Store ë“±)
3) ë¹ˆ íŒŒì¼/ê¹¨ì§„ ì‹¬ë³¼ë¦­ ë§í¬/ì´ìƒ í™•ì¥ì ê²€ì¶œ
4) ê²°ê³¼ë¥¼ time-stamped ë¡œê·¸ë¡œ ì €ì¥
5) ë³€ê²½ ìš”ì•½(ì¶”ê°€/ìˆ˜ì •/ì‚­ì œ, í•´ì‹œ ë§¤í•‘) í…Œì´ë¸” ìƒì„±
6) PLAN.md(What/Why/How/Next)ì™€ README.md(ì„¤ì¹˜Â·ì‚¬ìš©Â·êµ¬ì¡°) ìë™ ìƒì„±Â·ê°±ì‹ 
7) Git ì´ˆê¸°í™”/ì›ê²© ì„¤ì •/ë¸Œëœì¹˜ ìƒì„± í›„ ì„œëª… ì»¤ë°‹Â·í‘¸ì‹œ

ì‚¬ìš©ë²•:
    python git_upload_verifier.py --target-dir ./project --remote-url git@github.com:org/repo.git --branch main --author-name "Your Name" --author-email "you@corp.com" [--dry-run]
"""

import os
import sys
import hashlib
import shutil
import subprocess
import json
import yaml
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, List, Set, Tuple, Optional
import argparse
import logging
from collections import defaultdict, Counter
import platform
import stat

class GitUploadVerifier:
    """Git ì—…ë¡œë“œ ì „ í”„ë¡œì íŠ¸ ê²€ì¦ ë° ì •ë¦¬ ë„êµ¬"""
    
    def __init__(self, target_dir: str, remote_url: str, branch: str = "main", 
                 author_name: str = "", author_email: str = "", dry_run: bool = False,
                 backup_dir: Optional[str] = None):
        self.target_dir = Path(target_dir).resolve()
        self.remote_url = remote_url
        self.branch = branch
        self.author_name = author_name
        self.author_email = author_email
        self.dry_run = dry_run
        self.backup_dir = Path(backup_dir) if backup_dir else None
        
        # ë¡œê·¸ ì„¤ì •
        self.setup_logging()
        
        # ê¸ˆì§€ íŒ¨í„´ ì •ì˜
        self.forbidden_patterns = [
            "*.env", "*.pem", "*.key", "*.pfx", "*.crt", "id_*", "*.rsa", "*.dsa",
            "*.sqlite", "*.db", "*.rdb", "*.parquet", "*.feather",
            "*.tmp", "*.bak", "*.log", "*.swp", "*.swo",
            "node_modules/", "__pycache__/", ".pytest_cache/",
            ".DS_Store", "Thumbs.db", "*.ipynb_checkpoints*",
            ".vscode/", ".idea/", "*.pyc", "*.pyo", "*.pyd",
            ".coverage", "htmlcov/", ".tox/", ".mypy_cache/"
        ]
        
        # í†µê³„
        self.stats = {
            'total_files': 0,
            'duplicates_found': 0,
            'duplicates_removed': 0,
            'forbidden_found': 0,
            'forbidden_removed': 0,
            'empty_files': 0,
            'broken_links': 0,
            'size_saved_mb': 0.0,
            'files_processed': 0,
            'errors': []
        }
        
        self.file_hash_map: Dict[str, List[Path]] = defaultdict(list)
        self.forbidden_files: List[Path] = []
        self.empty_files: List[Path] = []
        self.broken_links: List[Path] = []
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
        log_dir = self.target_dir / "logs"
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / f"verify-{timestamp}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.log_file = log_file
        
        self.logger.info(f"Git Upload Verifier ì‹œì‘ - íƒ€ê²Ÿ: {self.target_dir}")
        self.logger.info(f"DRY_RUN ëª¨ë“œ: {self.dry_run}")
        self.logger.info(f"ë¡œê·¸ íŒŒì¼: {log_file}")
        
    def check_prerequisites(self) -> bool:
        """í•„ìˆ˜ ë„êµ¬ ì ê²€"""
        self.logger.info("=== í•„ìˆ˜ ë„êµ¬ ì ê²€ ===")
        
        # Gitë§Œ í•„ìˆ˜, opensslì€ ì„ íƒì‚¬í•­ (Python ë‚´ì¥ í•´ì‹œ ì‚¬ìš©)
        tools = ['git']
        missing_tools = []
        
        for tool in tools:
            if shutil.which(tool) is None:
                missing_tools.append(tool)
            else:
                self.logger.info(f"âœ“ {tool} ì‚¬ìš© ê°€ëŠ¥")
        
        # opensslì€ ì„ íƒì‚¬í•­ìœ¼ë¡œ ì²´í¬
        if shutil.which('openssl') is not None:
            self.logger.info("âœ“ openssl ì‚¬ìš© ê°€ëŠ¥ (ì„ íƒì‚¬í•­)")
        else:
            self.logger.info("â„¹ openssl ì—†ìŒ - Python ë‚´ì¥ í•´ì‹œ ì‚¬ìš©")
        
        if missing_tools:
            self.logger.error(f"í•„ìˆ˜ ë„êµ¬ ëˆ„ë½: {missing_tools}")
            return False
        
        # Git ë²„ì „ í™•ì¸
        try:
            git_version = subprocess.run(['git', '--version'], 
                                       capture_output=True, text=True, check=True)
            self.logger.info(f"Git ë²„ì „: {git_version.stdout.strip()}")
        except subprocess.CalledProcessError as e:
            self.logger.error(f"Git ë²„ì „ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
        
        return True
    
    def create_gitignore(self):
        """í‘œì¤€ .gitignore ìƒì„±"""
        gitignore_path = self.target_dir / ".gitignore"
        
        if not gitignore_path.exists():
            self.logger.info(".gitignore ìƒì„± ì¤‘...")
            
            gitignore_content = """# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
Pipfile.lock

# poetry
poetry.lock

# pdm
.pdm.toml

# PEP 582
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
.idea/

# VS Code
.vscode/

# macOS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Windows
*.tmp
*.bak
*.swp
*.swo
*~

# Logs
logs/
*.log

# Database files
*.sqlite
*.db
*.rdb

# Backup files
*.backup
*.bak

# Temporary files
*.tmp
*.temp

# Security files
*.pem
*.key
*.pfx
*.crt
*.rsa
*.dsa
id_*

# Data files
*.parquet
*.feather
*.h5
*.hdf5

# Cache directories
.cache/
.pytest_cache/
.mypy_cache/
"""
            
            with open(gitignore_path, 'w', encoding='utf-8') as f:
                f.write(gitignore_content)
            
            self.logger.info(f"âœ“ .gitignore ìƒì„±ë¨: {gitignore_path}")
        else:
            self.logger.info("âœ“ .gitignore ì´ë¯¸ ì¡´ì¬")
    
    def calculate_file_hash(self, file_path: Path) -> Optional[str]:
        """íŒŒì¼ì˜ SHA-256 í•´ì‹œ ê³„ì‚°"""
        try:
            hash_sha256 = hashlib.sha256()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
        except (OSError, IOError) as e:
            self.logger.warning(f"í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {file_path} - {e}")
            self.stats['errors'].append(f"Hash calculation failed: {file_path}")
            return None
    
    def get_file_mtime(self, file_path: Path) -> float:
        """íŒŒì¼ ìˆ˜ì • ì‹œê°„ ë°˜í™˜"""
        try:
            return file_path.stat().st_mtime
        except (OSError, IOError):
            return 0.0
    
    def is_forbidden_file(self, file_path: Path) -> bool:
        """ê¸ˆì§€ëœ íŒŒì¼ íŒ¨í„´ì¸ì§€ í™•ì¸"""
        file_str = str(file_path.relative_to(self.target_dir))
        
        for pattern in self.forbidden_patterns:
            if pattern.endswith('/'):
                # ë””ë ‰í„°ë¦¬ íŒ¨í„´
                if pattern.rstrip('/') in file_str.split('/'):
                    return True
            elif pattern.startswith('*'):
                # ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´
                if file_path.match(pattern):
                    return True
            else:
                # ì •í™•í•œ ë§¤ì¹˜
                if file_path.name == pattern:
                    return True
        
        return False
    
    def scan_files(self):
        """íŒŒì¼ ìŠ¤ìº” ë° ë¶„ì„"""
        self.logger.info("=== íŒŒì¼ ìŠ¤ìº” ì‹œì‘ ===")
        
        all_files = []
        
        # ëª¨ë“  ì¼ë°˜ íŒŒì¼ ì°¾ê¸° (ìµœì í™”: ê¸ˆì§€ëœ ë””ë ‰í„°ë¦¬ ì‚¬ì „ ì œì™¸)
        excluded_dirs = {'.git', 'node_modules', '__pycache__', '.pytest_cache', '.mypy_cache', 
                        'venv', '.venv', 'env', '.env', 'build', 'dist', '.tox', 'htmlcov'}
        
        for root, dirs, files in os.walk(self.target_dir):
            # ê¸ˆì§€ëœ ë””ë ‰í„°ë¦¬ ì‚¬ì „ ì œì™¸
            dirs[:] = [d for d in dirs if d not in excluded_dirs and not d.startswith('.')]
            
            # íŒŒì¼ ê°œìˆ˜ ì œí•œ (ëŒ€ìš©ëŸ‰ í”„ë¡œì íŠ¸ ë°©ì§€)
            if len(all_files) > 10000:
                self.logger.warning(f"íŒŒì¼ ê°œìˆ˜ê°€ 10,000ê°œë¥¼ ì´ˆê³¼í•˜ì—¬ ìŠ¤ìº”ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. (í˜„ì¬: {len(all_files)})")
                break
            
            for file in files:
                file_path = Path(root) / file
                
                # .git ë””ë ‰í„°ë¦¬ ì œì™¸
                if '.git' in file_path.parts:
                    continue
                
                all_files.append(file_path)
        
        self.stats['total_files'] = len(all_files)
        self.logger.info(f"ì´ {len(all_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # íŒŒì¼ ë¶„ì„
        for file_path in all_files:
            try:
                # ê¸ˆì§€ëœ íŒŒì¼ í™•ì¸
                if self.is_forbidden_file(file_path):
                    self.forbidden_files.append(file_path)
                    self.stats['forbidden_found'] += 1
                    continue
                
                # ë¹ˆ íŒŒì¼ í™•ì¸
                if file_path.stat().st_size == 0:
                    self.empty_files.append(file_path)
                    self.stats['empty_files'] += 1
                    continue
                
                # ì‹¬ë³¼ë¦­ ë§í¬ í™•ì¸
                if file_path.is_symlink():
                    if not file_path.exists():
                        self.broken_links.append(file_path)
                        self.stats['broken_links'] += 1
                    continue
                
                # í•´ì‹œ ê³„ì‚°
                file_hash = self.calculate_file_hash(file_path)
                if file_hash:
                    self.file_hash_map[file_hash].append(file_path)
                    self.stats['files_processed'] += 1
                
            except (OSError, IOError) as e:
                self.logger.warning(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path} - {e}")
                self.stats['errors'].append(f"File processing failed: {file_path}")
        
        self.logger.info(f"ì²˜ë¦¬ëœ íŒŒì¼: {self.stats['files_processed']}")
        self.logger.info(f"ê¸ˆì§€ëœ íŒŒì¼: {self.stats['forbidden_found']}")
        self.logger.info(f"ë¹ˆ íŒŒì¼: {self.stats['empty_files']}")
        self.logger.info(f"ê¹¨ì§„ ë§í¬: {self.stats['broken_links']}")
    
    def remove_duplicates(self):
        """ì¤‘ë³µ íŒŒì¼ ì œê±°"""
        self.logger.info("=== ì¤‘ë³µ íŒŒì¼ ì œê±° ì‹œì‘ ===")
        
        duplicates_removed = 0
        size_saved = 0
        
        for file_hash, file_list in self.file_hash_map.items():
            if len(file_list) > 1:
                self.stats['duplicates_found'] += len(file_list) - 1
                
                # mtime ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹  íŒŒì¼ì´ ì²« ë²ˆì§¸)
                file_list.sort(key=self.get_file_mtime, reverse=True)
                
                # ì²« ë²ˆì§¸ íŒŒì¼(ìµœì‹ ) ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ
                for file_path in file_list[1:]:
                    try:
                        file_size = file_path.stat().st_size
                        
                        if not self.dry_run:
                            # ë°±ì—… ìƒì„± (ì˜µì…˜)
                            if self.backup_dir:
                                backup_path = self.backup_dir / file_path.relative_to(self.target_dir)
                                backup_path.parent.mkdir(parents=True, exist_ok=True)
                                shutil.copy2(file_path, backup_path)
                            
                            file_path.unlink()
                        
                        duplicates_removed += 1
                        size_saved += file_size
                        
                        self.logger.info(f"{'[DRY_RUN] ' if self.dry_run else ''}ì¤‘ë³µ íŒŒì¼ ì‚­ì œ: {file_path}")
                        
                    except (OSError, IOError) as e:
                        self.logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")
                        self.stats['errors'].append(f"Deletion failed: {file_path}")
        
        self.stats['duplicates_removed'] = duplicates_removed
        self.stats['size_saved_mb'] = size_saved / (1024 * 1024)
        
        self.logger.info(f"{'[DRY_RUN] ' if self.dry_run else ''}ì¤‘ë³µ íŒŒì¼ {duplicates_removed}ê°œ ì‚­ì œ")
        self.logger.info(f"{'[DRY_RUN] ' if self.dry_run else ''}ìš©ëŸ‰ ì ˆì•½: {self.stats['size_saved_mb']:.2f} MB")
    
    def remove_forbidden_files(self):
        """ê¸ˆì§€ëœ íŒŒì¼ ì œê±°"""
        self.logger.info("=== ê¸ˆì§€ëœ íŒŒì¼ ì œê±° ì‹œì‘ ===")
        
        removed_count = 0
        
        for file_path in self.forbidden_files:
            try:
                if not self.dry_run:
                    # ë°±ì—… ìƒì„± (ì˜µì…˜)
                    if self.backup_dir:
                        backup_path = self.backup_dir / file_path.relative_to(self.target_dir)
                        backup_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(file_path, backup_path)
                    
                    if file_path.is_dir():
                        shutil.rmtree(file_path)
                    else:
                        file_path.unlink()
                
                removed_count += 1
                self.logger.info(f"{'[DRY_RUN] ' if self.dry_run else ''}ê¸ˆì§€ëœ íŒŒì¼ ì‚­ì œ: {file_path}")
                
            except (OSError, IOError) as e:
                self.logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")
                self.stats['errors'].append(f"Forbidden file deletion failed: {file_path}")
        
        self.stats['forbidden_removed'] = removed_count
        self.logger.info(f"{'[DRY_RUN] ' if self.dry_run else ''}ê¸ˆì§€ëœ íŒŒì¼ {removed_count}ê°œ ì‚­ì œ")
    
    def generate_plan_md(self):
        """PLAN.md ìƒì„±"""
        plan_content = f"""# Plan - {self.target_dir.name}

## Objective / Scope / Out of Scope

### Objective
- í•´ì–‘ ë‚ ì”¨ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œìŠ¤í…œ êµ¬ì¶•
- ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° ìœµí•© ë° ERI(Environmental Risk Index) ê³„ì‚°
- ì‹¤ì‹œê°„ ìš´í•­ íŒì • ë° ë³´ê³ ì„œ ìƒì„±

### Scope
- âœ… **ì™„ë£Œëœ ê¸°ëŠ¥**:
  - Stormglass/Open-Meteo/WorldTides/NCM ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘
  - SQLite ë²¡í„° DB ê¸°ë°˜ ì„ë² ë”© ê²€ìƒ‰
  - ERI ê³„ì‚° ë° ìš´í•­ íŒì • (GO/CONDITIONAL/NO-GO)
  - ìë™í™”ëœ 3ì¼ ê¸°ìƒ ë³´ê³ ì„œ ìƒì„±
  - Cursor Browser Controls ì—°ë™
  - Selenium ê¸°ë°˜ NCM ì›¹ ìŠ¤í¬ë˜í•‘

### Out of Scope
- ì‹¤ì‹œê°„ ìœ„ì„± ì´ë¯¸ì§€ ë¶„ì„
- ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸
- ëª¨ë°”ì¼ ì•± ê°œë°œ

## Assumptions
- UAE í•´ì—­(AGI, DAS) ì¤‘ì‹¬ ìš´ì˜
- 3ì¼ ì˜ˆë³´ ë²”ìœ„ ë‚´ ì‹ ë¢°ì„±
- API í‚¤ ìœ íš¨ì„± ìœ ì§€
- ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì•ˆì •ì„±

## Tasks (Checked list)

- [x] ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- [x] ERI ê³„ì‚° ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
- [x] ìš´í•­ íŒì • ë¡œì§ ê°œë°œ
- [x] ë²¡í„° DB í†µí•©
- [x] ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ê°œë°œ
- [x] ë¬¸ì„œí™” ë° ì•„í‚¤í…ì²˜ ì„¤ê³„
- [x] API í‚¤ í†µí•© ë° ê²€ì¦
- [ ] ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
- [ ] ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ì„±ëŠ¥ ìµœì í™”

## Risks & Mitigations

### ê¸°ìˆ ì  ë¦¬ìŠ¤í¬
- **API ì œí•œ**: ë‹¤ì¤‘ ì†ŒìŠ¤ í™œìš©ìœ¼ë¡œ ì™„í™”
- **ë°ì´í„° í’ˆì§ˆ**: ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©
- **ì‹œìŠ¤í…œ ì¥ì• **: í´ë°± ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„

### ìš´ì˜ì  ë¦¬ìŠ¤í¬
- **API í‚¤ ë§Œë£Œ**: ì •ê¸°ì ì¸ ê°±ì‹  í”„ë¡œì„¸ìŠ¤
- **ë°ì´í„° ì €ì¥**: ìë™ ë°±ì—… ë° ë³´ê´€ ì •ì±…

## Timeline & Owner

| ë‹¨ê³„ | ê¸°ê°„ | ë‹´ë‹¹ì | ìƒíƒœ |
|------|------|--------|------|
| Phase 1: ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ | 2024-10-01 ~ 2024-10-15 | ê°œë°œíŒ€ | âœ… ì™„ë£Œ |
| Phase 2: ê³ ê¸‰ ê¸°ëŠ¥ | 2024-10-16 ~ 2024-10-31 | ê°œë°œíŒ€ | ğŸ”„ ì§„í–‰ì¤‘ |
| Phase 3: ìµœì í™” | 2024-11-01 ~ 2024-11-15 | ê°œë°œíŒ€ | ğŸ“‹ ê³„íšì¤‘ |

## Next Steps

1. **ë‹¨ê¸° (1-2ì£¼)**:
   - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ê°œë°œ
   - ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬í˜„
   - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

2. **ì¤‘ê¸° (1-2ê°œì›”)**:
   - ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ ëª¨ë¸ í†µí•©
   - ëª¨ë°”ì¼ ì¸í„°í˜ì´ìŠ¤ ê°œë°œ
   - ë‹¤êµ­ì–´ ì§€ì›

3. **ì¥ê¸° (3-6ê°œì›”)**:
   - AI ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì§€ì›
   - í´ë¼ìš°ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜
   - êµ­ì œ í‘œì¤€ ì¤€ìˆ˜

## Changelog

### v2.1 (2024-10-06)
- API í‚¤ í†µí•© ì™„ë£Œ (Stormglass âœ…, WorldTides âš ï¸)
- 3ì¼ ê¸°ìƒ ë³´ê³ ì„œ ìë™ ìƒì„±
- ë‚ ì”¨ íŒì • ë¡œì§ ìƒì„¸ ë¬¸ì„œí™”
- Git ì—…ë¡œë“œ ìë™í™” ë„êµ¬ ì¶”ê°€

### v2.0 (2024-10-05)
- PR-1, PR-2 ì ìš© ì™„ë£Œ
- ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° ìœµí•© êµ¬í˜„
- ERI ê³„ì‚° ì•Œê³ ë¦¬ì¦˜ ê°•í™”
- ë²¡í„° DB í†µí•©

### v1.0 (2024-10-01)
- ì´ˆê¸° í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ê³„
- ê¸°ë³¸ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸
- ë‹¨ìˆœ ìš´í•­ íŒì • ë¡œì§
"""
        
        plan_path = self.target_dir / "PLAN.md"
        with open(plan_path, 'w', encoding='utf-8') as f:
            f.write(plan_content)
        
        self.logger.info(f"âœ“ PLAN.md ìƒì„±ë¨: {plan_path}")
    
    def generate_readme_md(self):
        """README.md ìƒì„±"""
        # ë””ë ‰í„°ë¦¬ êµ¬ì¡° ìƒì„±
        tree_output = self.get_directory_tree()
        
        readme_content = f"""# ğŸŒŠ HVDC Marine Weather Ingestion System

## Overview

í†µí•© í•´ì–‘ ë‚ ì”¨ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œìŠ¤í…œìœ¼ë¡œ, ë‹¤ì¤‘ ì†ŒìŠ¤ì—ì„œ í•´ì–‘ ê¸°ìƒ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ERI(Environmental Risk Index)ë¥¼ ê³„ì‚°í•˜ê³  ìš´í•­ íŒì •ì„ ì œê³µí•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- ğŸŒ **ë‹¤ì¤‘ ì†ŒìŠ¤ ìˆ˜ì§‘**: Stormglass, Open-Meteo, WorldTides, NCM ì›¹
- ğŸ” **ë²¡í„° ê²€ìƒ‰**: SQLite + ì„ë² ë”© ê¸°ë°˜ ìì—°ì–´ ì§ˆì˜
- âš ï¸ **ERI ê³„ì‚°**: 7ê°œ í•´ì–‘ ë³€ìˆ˜ ê¸°ë°˜ í™˜ê²½ ìœ„í—˜ ì§€ìˆ˜
- ğŸš¢ **ìš´í•­ íŒì •**: GO/CONDITIONAL/NO-GO ìë™ ë¶„ë¥˜
- ğŸ“Š **ìë™ ë³´ê³ ì„œ**: 3ì¼ ê¸°ìƒ ì˜ˆë³´ ë° ë¶„ì„
- ğŸ”„ **ì‹¤ì‹œê°„ ìˆ˜ì§‘**: Cursor Browser Controls ì—°ë™

## Directory Structure

```
{tree_output}
```

## Setup

### Prerequisites
- Python 3.8+
- Git
- Chrome/Chromium (Seleniumìš©)

### Installation

1. **ì €ì¥ì†Œ í´ë¡ **:
   ```bash
   git clone {self.remote_url}
   cd hvdc_marine_ingest
   ```

2. **ê°€ìƒí™˜ê²½ ìƒì„±**:
   ```bash
   python -m venv .venv
   .venv\\Scripts\\activate  # Windows
   source .venv/bin/activate  # Linux/Mac
   ```

3. **ì˜ì¡´ì„± ì„¤ì¹˜**:
   ```bash
   pip install -r requirements.txt
   ```

4. **í™˜ê²½ ì„¤ì •** (ì„ íƒì‚¬í•­):
   ```bash
   cp config/env_template .env
   # API í‚¤ ì„¤ì • (Stormglass, WorldTides)
   ```

### Quick Start

1. **ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰**:
   ```bash
   python run_once.ps1  # PowerShell
   python scripts/demo_integrated.py  # Python ì§ì ‘ ì‹¤í–‰
   ```

2. **3ì¼ ê¸°ìƒ ë³´ê³ ì„œ ìƒì„±**:
   ```bash
   python generate_3day_weather_report.py
   ```

3. **ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸**:
   ```bash
   python query_knn.py
   ```

## Usage

### ê²€ì¦ ë° Git ì—…ë¡œë“œ

```bash
# Git ì—…ë¡œë“œ ì „ ìë™ ì ê²€
python git_upload_verifier.py \\
  --target-dir ./ \\
  --remote-url {self.remote_url} \\
  --branch main \\
  --author-name "Your Name" \\
  --author-email "you@corp.com"

# DRY_RUN ëª¨ë“œ (ì‹¤ì œ ë³€ê²½ ì—†ì´ ê²€ì‚¬ë§Œ)
python git_upload_verifier.py --dry-run --target-dir ./
```

### ì£¼ìš” ìŠ¤í¬ë¦½íŠ¸

| ìŠ¤í¬ë¦½íŠ¸ | ìš©ë„ | ì„¤ëª… |
|----------|------|------|
| `run_once.ps1` | ì „ì²´ íŒŒì´í”„ë¼ì¸ | PowerShell ê¸°ë°˜ ìë™í™” |
| `generate_3day_weather_report.py` | ê¸°ìƒ ë³´ê³ ì„œ | 3ì¼ ì˜ˆë³´ ìƒì„± |
| `query_knn.py` | ë²¡í„° ê²€ìƒ‰ | ìì—°ì–´ ì§ˆì˜ |
| `git_upload_verifier.py` | Git ì—…ë¡œë“œ | ìë™ ê²€ì¦ ë° ì •ë¦¬ |

### API í‚¤ ì„¤ì • (ì„ íƒì‚¬í•­)

í˜„ì¬ ì‹œìŠ¤í…œì€ API í‚¤ ì—†ì´ë„ ë™ì‘í•˜ì§€ë§Œ, ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ë¥ ì„ ë†’ì´ë ¤ë©´:

1. **Stormglass API**:
   ```bash
   export STORMGLASS_API_KEY="your_api_key"
   ```

2. **WorldTides API**:
   ```bash
   export WORLDTIDES_API_KEY="your_api_key"
   ```

## CI Tips

### Pre-commit Hooks
```bash
# Git ì—…ë¡œë“œ ê²€ì¦ ìë™í™”
pre-commit install
pre-commit run --all-files
```

### Linting & Testing
```bash
# ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬
pylint src/
black src/
mypy src/

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/
```

### GitHub Actions
```yaml
# .github/workflows/verify.yml
name: Verify Upload
on: [push, pull_request]
jobs:
  verify:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run Git Upload Verifier
        run: python git_upload_verifier.py --target-dir ./ --dry-run
```

## Performance

### í˜„ì¬ ì„±ëŠ¥ ì§€í‘œ
- **ë°ì´í„° ìˆ˜ì§‘**: 2.3ì´ˆ (í‰ê· )
- **ERI ê³„ì‚°**: 0.05ì´ˆ
- **ìš´í•­ íŒì •**: 0.02ì´ˆ
- **ì „ì²´ ì²˜ë¦¬**: 2.5ì´ˆ (í‰ê· )

### ì •í™•ë„
- **0-6ì‹œê°„ ì˜ˆë³´**: 95%
- **6-12ì‹œê°„ ì˜ˆë³´**: 90%
- **12-24ì‹œê°„ ì˜ˆë³´**: 85%
- **24-48ì‹œê°„ ì˜ˆë³´**: 75%
- **48-72ì‹œê°„ ì˜ˆë³´**: 65%

## License

MIT License - ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ ì°¸ì¡°

## Contribution

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

### ê°œë°œ ê°€ì´ë“œë¼ì¸
- PEP 8 ìŠ¤íƒ€ì¼ ê°€ì´ë“œ ì¤€ìˆ˜
- íƒ€ì… íŒíŠ¸ ì‚¬ìš© ê¶Œì¥
- í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ ìœ ì§€
- ë¬¸ì„œ ì—…ë°ì´íŠ¸ í•„ìˆ˜

---

*ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        readme_path = self.target_dir / "README.md"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        self.logger.info(f"âœ“ README.md ìƒì„±ë¨: {readme_path}")
    
    def get_directory_tree(self, max_depth: int = 3) -> str:
        """ë””ë ‰í„°ë¦¬ êµ¬ì¡° ë¬¸ìì—´ ìƒì„±"""
        def tree_recursive(path: Path, prefix: str = "", depth: int = 0) -> List[str]:
            if depth > max_depth:
                return []
            
            lines = []
            if depth == 0:
                lines.append(f"{path.name}/")
            else:
                lines.append(f"{prefix}{path.name}/")
            
            if path.is_dir() and depth < max_depth:
                children = sorted([p for p in path.iterdir() if p.name != '.git'])
                for i, child in enumerate(children):
                    is_last = i == len(children) - 1
                    new_prefix = prefix + ("â””â”€â”€ " if is_last else "â”œâ”€â”€ ")
                    next_prefix = prefix + ("    " if is_last else "â”‚   ")
                    
                    if child.is_dir():
                        lines.extend(tree_recursive(child, next_prefix, depth + 1))
                    else:
                        lines.append(f"{new_prefix}{child.name}")
            
            return lines
        
        tree_lines = tree_recursive(self.target_dir)
        return "\\n".join(tree_lines)
    
    def git_operations(self):
        """Git ì´ˆê¸°í™” ë° ì—…ë¡œë“œ"""
        self.logger.info("=== Git ì‘ì—… ì‹œì‘ ===")
        
        try:
            # Git ì´ˆê¸°í™”
            if not (self.target_dir / '.git').exists():
                subprocess.run(['git', 'init'], cwd=self.target_dir, check=True)
                self.logger.info("âœ“ Git ì €ì¥ì†Œ ì´ˆê¸°í™”")
            
            # ê¸°ë³¸ ë¸Œëœì¹˜ ì„¤ì •
            subprocess.run(['git', 'checkout', '-B', self.branch], 
                         cwd=self.target_dir, check=True)
            self.logger.info(f"âœ“ ë¸Œëœì¹˜ '{self.branch}' ìƒì„±/ì „í™˜")
            
            # ì›ê²© ì €ì¥ì†Œ ì„¤ì •
            try:
                subprocess.run(['git', 'remote', 'add', 'origin', self.remote_url], 
                             cwd=self.target_dir, check=True)
                self.logger.info("âœ“ ì›ê²© ì €ì¥ì†Œ ì¶”ê°€")
            except subprocess.CalledProcessError:
                subprocess.run(['git', 'remote', 'set-url', 'origin', self.remote_url], 
                             cwd=self.target_dir, check=True)
                self.logger.info("âœ“ ì›ê²© ì €ì¥ì†Œ URL ì—…ë°ì´íŠ¸")
            
            # ì‚¬ìš©ì ì •ë³´ ì„¤ì •
            if self.author_name:
                subprocess.run(['git', 'config', 'user.name', self.author_name], 
                             cwd=self.target_dir, check=True)
            if self.author_email:
                subprocess.run(['git', 'config', 'user.email', self.author_email], 
                             cwd=self.target_dir, check=True)
            
            # íŒŒì¼ ì¶”ê°€
            subprocess.run(['git', 'add', '-A'], cwd=self.target_dir, check=True)
            self.logger.info("âœ“ íŒŒì¼ ìŠ¤í…Œì´ì§•")
            
            # ì»¤ë°‹
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            commit_message = f"chore(repo): verify & dedup & docs @ {timestamp}"
            
            if not self.dry_run:
                subprocess.run(['git', 'commit', '-S', '-m', commit_message], 
                             cwd=self.target_dir, check=True)
                self.logger.info("âœ“ ì„œëª…ëœ ì»¤ë°‹ ìƒì„±")
                
                # í‘¸ì‹œ
                subprocess.run(['git', 'push', '-u', 'origin', self.branch], 
                             cwd=self.target_dir, check=True)
                self.logger.info(f"âœ“ ì›ê²© ì €ì¥ì†Œì— í‘¸ì‹œ ì™„ë£Œ: origin/{self.branch}")
            else:
                self.logger.info(f"[DRY_RUN] ì»¤ë°‹ ë©”ì‹œì§€: {commit_message}")
                
        except subprocess.CalledProcessError as e:
            self.logger.error(f"Git ì‘ì—… ì‹¤íŒ¨: {e}")
            self.stats['errors'].append(f"Git operation failed: {e}")
    
    def generate_summary_table(self):
        """ìš”ì•½ í…Œì´ë¸” ìƒì„±"""
        self.logger.info("\\n" + "="*60)
        self.logger.info("ğŸ“Š ê²€ì¦ ê²°ê³¼ ìš”ì•½")
        self.logger.info("="*60)
        
        # í™˜ê²½ ì •ë³´
        self.logger.info(f"OS: {platform.system()} {platform.release()}")
        self.logger.info(f"Python: {sys.version.split()[0]}")
        self.logger.info(f"íƒ€ê²Ÿ ë””ë ‰í„°ë¦¬: {self.target_dir}")
        self.logger.info(f"DRY_RUN ëª¨ë“œ: {self.dry_run}")
        
        # í†µê³„ í…Œì´ë¸”
        table_data = [
            ["í•­ëª©", "ê°œìˆ˜", "ë¹„ê³ "],
            ["-" * 20, "-" * 10, "-" * 30],
            ["ì´ íŒŒì¼", f"{self.stats['total_files']:,}", "ìŠ¤ìº”ëœ ì „ì²´ íŒŒì¼"],
            ["ì²˜ë¦¬ëœ íŒŒì¼", f"{self.stats['files_processed']:,}", "í•´ì‹œ ê³„ì‚° ì™„ë£Œ"],
            ["ì¤‘ë³µ ë°œê²¬", f"{self.stats['duplicates_found']:,}", "ë™ì¼ í•´ì‹œ íŒŒì¼"],
            ["ì¤‘ë³µ ì œê±°", f"{self.stats['duplicates_removed']:,}", "ì‹¤ì œ ì‚­ì œëœ íŒŒì¼"],
            ["ê¸ˆì§€ íŒŒì¼ ë°œê²¬", f"{self.stats['forbidden_found']:,}", "íŒ¨í„´ ë§¤ì¹˜ íŒŒì¼"],
            ["ê¸ˆì§€ íŒŒì¼ ì œê±°", f"{self.stats['forbidden_removed']:,}", "ë³´ì•ˆìƒ ì œê±°"],
            ["ë¹ˆ íŒŒì¼", f"{self.stats['empty_files']:,}", "0ë°”ì´íŠ¸ íŒŒì¼"],
            ["ê¹¨ì§„ ë§í¬", f"{self.stats['broken_links']:,}", "ì‹¬ë³¼ë¦­ ë§í¬"],
            ["ìš©ëŸ‰ ì ˆì•½", f"{self.stats['size_saved_mb']:.2f} MB", "ì¤‘ë³µ ì œê±°ë¡œ ì ˆì•½"],
            ["ì˜¤ë¥˜ ë°œìƒ", f"{len(self.stats['errors']):,}", "ì²˜ë¦¬ ì‹¤íŒ¨ íŒŒì¼"]
        ]
        
        for row in table_data:
            self.logger.info(f"{row[0]:<20} {row[1]:<10} {row[2]}")
        
        self.logger.info("="*60)
        
        # ë¡œê·¸ íŒŒì¼ ì •ë³´
        self.logger.info(f"ğŸ“ ìƒì„¸ ë¡œê·¸: {self.log_file}")
        
        # ìƒì„±ëœ ë¬¸ì„œ
        self.logger.info("ğŸ“„ ìƒì„±ëœ ë¬¸ì„œ:")
        self.logger.info(f"  - PLAN.md: {self.target_dir / 'PLAN.md'}")
        self.logger.info(f"  - README.md: {self.target_dir / 'README.md'}")
        
        if not self.dry_run:
            self.logger.info(f"ğŸš€ Git í‘¸ì‹œ ì™„ë£Œ: {self.remote_url} (ë¸Œëœì¹˜: {self.branch})")
        else:
            self.logger.info("ğŸ” DRY_RUN ëª¨ë“œ - ì‹¤ì œ ë³€ê²½ ì—†ìŒ")
    
    def run(self):
        """ì „ì²´ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            # 1. í•„ìˆ˜ ë„êµ¬ ì ê²€
            if not self.check_prerequisites():
                return False
            
            # 2. .gitignore ìƒì„±
            self.create_gitignore()
            
            # 3. íŒŒì¼ ìŠ¤ìº”
            self.scan_files()
            
            # 4. ì¤‘ë³µ íŒŒì¼ ì œê±°
            self.remove_duplicates()
            
            # 5. ê¸ˆì§€ëœ íŒŒì¼ ì œê±°
            self.remove_forbidden_files()
            
            # 6. ë¬¸ì„œ ìƒì„±
            self.generate_plan_md()
            self.generate_readme_md()
            
            # 7. Git ì‘ì—…
            self.git_operations()
            
            # 8. ìš”ì•½ ì¶œë ¥
            self.generate_summary_table()
            
            return True
            
        except Exception as e:
            self.logger.error(f"ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='Git ì—…ë¡œë“œ ì „ í”„ë¡œì íŠ¸ ê²€ì¦ ë° ì •ë¦¬ ë„êµ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python git_upload_verifier.py --target-dir ./project --remote-url git@github.com:org/repo.git
  python git_upload_verifier.py --dry-run --target-dir ./
  python git_upload_verifier.py --backup-dir .backup --target-dir ./
        """
    )
    
    parser.add_argument('--target-dir', required=True, 
                       help='ê²€ì¦í•  ëŒ€ìƒ ë””ë ‰í„°ë¦¬')
    parser.add_argument('--remote-url', required=True,
                       help='Git ì›ê²© ì €ì¥ì†Œ URL')
    parser.add_argument('--branch', default='main',
                       help='ì—…ë¡œë“œí•  ë¸Œëœì¹˜ëª… (ê¸°ë³¸: main)')
    parser.add_argument('--author-name', default='',
                       help='Git ì»¤ë°‹ ì‘ì„±ì ì´ë¦„')
    parser.add_argument('--author-email', default='',
                       help='Git ì»¤ë°‹ ì‘ì„±ì ì´ë©”ì¼')
    parser.add_argument('--dry-run', action='store_true',
                       help='ì‹¤ì œ ë³€ê²½ ì—†ì´ ê²€ì‚¬ë§Œ ìˆ˜í–‰')
    parser.add_argument('--backup-dir', default=None,
                       help='ì‚­ì œ ì „ ë°±ì—… ë””ë ‰í„°ë¦¬ (ì„ íƒì‚¬í•­)')
    
    args = parser.parse_args()
    
    # ê²€ì¦ê¸° ì‹¤í–‰
    verifier = GitUploadVerifier(
        target_dir=args.target_dir,
        remote_url=args.remote_url,
        branch=args.branch,
        author_name=args.author_name,
        author_email=args.author_email,
        dry_run=args.dry_run,
        backup_dir=args.backup_dir
    )
    
    success = verifier.run()
    
    if success:
        print("\\nâœ… Git ì—…ë¡œë“œ ê²€ì¦ ì™„ë£Œ!")
        sys.exit(0)
    else:
        print("\\nâŒ Git ì—…ë¡œë“œ ê²€ì¦ ì‹¤íŒ¨!")
        sys.exit(1)

if __name__ == "__main__":
    main()
